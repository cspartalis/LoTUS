{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cspartalis/anaconda3/envs/MaUn/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from data_utils import UnlearningDataLoader\n",
    "\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from config import set_config\n",
    "from data_utils import UnlearningDataLoader\n",
    "from eval import (\n",
    "    compute_accuracy,\n",
    "    get_forgetting_rate,\n",
    "    get_js_div,\n",
    "    get_l2_params_distance,\n",
    "    mia,\n",
    ")\n",
    "from mlflow_utils import mlflow_tracking_uri\n",
    "from models import VGG19, AllCNN, ResNet18, ViT\n",
    "from seed import set_seed\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args\n",
    "run_id = \"1bcdd3b016d14404ab22c476184bff75\"\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# Load params from retraining run\n",
    "retrain_run = mlflow.get_run(run_id)\n",
    "seed = int(retrain_run.data.params[\"seed\"])\n",
    "dataset = retrain_run.data.params[\"dataset\"]\n",
    "model_str = retrain_run.data.params[\"model\"]\n",
    "batch_size = int(retrain_run.data.params[\"batch_size\"])\n",
    "epochs_to_retrain = int(retrain_run.data.metrics[\"best_epoch\"])\n",
    "loss_str = retrain_run.data.params[\"loss\"]\n",
    "optimizer_str = retrain_run.data.params[\"optimizer\"]\n",
    "momentum = float(retrain_run.data.params[\"momentum\"])\n",
    "weight_decay = float(retrain_run.data.params[\"weight_decay\"])\n",
    "acc_forget_retrain = int(retrain_run.data.metrics[\"acc_forget\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "UDL = UnlearningDataLoader(dataset, batch_size, seed)\n",
    "dl, dataset_sizes = UDL.load_data()\n",
    "num_classes = len(UDL.classes)\n",
    "input_channels = UDL.input_channels\n",
    "image_size = UDL.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 6/6 [00:01<00:00,  3.83it/s]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet18(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model architecture\n",
    "if model_str == \"resnet18\":\n",
    "    model = ResNet18(input_channels, num_classes)\n",
    "elif model_str == \"allcnn\":\n",
    "    model = AllCNN(input_channels, num_classes)\n",
    "elif model_str == \"vgg19\":\n",
    "    model = VGG19(input_channels, num_classes)\n",
    "elif model_str == \"vit\":\n",
    "    model = ViT(image_size=image_size, num_classes=num_classes)\n",
    "else:\n",
    "    raise ValueError(\"Model not supported\")\n",
    "# Load the original model\n",
    "model = mlflow.pytorch.load_model(f\"{retrain_run.info.artifact_uri}/original_model\")\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def original_lrp_fc_layer(model, dataloader):\n",
    "    # Define a hook function to get the activations\n",
    "    def get_activations_hook(module, input, output):\n",
    "        activations = input[0].detach().cpu().numpy()\n",
    "        activations_hook.append(activations)\n",
    "\n",
    "\n",
    "    activations_hook = []\n",
    "    model.fc.register_forward_hook(get_activations_hook)\n",
    "\n",
    "    for idx, (inputs, _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get the activations\n",
    "        batch_activations = activations_hook[idx]\n",
    "        batch_activations = torch.from_numpy(batch_activations).to(DEVICE)\n",
    "        # print(f\"batch_activations.shape: {batch_activations.shape}\")\n",
    "\n",
    "        T = torch.eye(outputs.size(-1)).to(DEVICE)\n",
    "        T = T[outputs.argmax(dim=1)] # Select the row from the identity matrix that corresponds to the outputs's highest logit\n",
    "\n",
    "        # Compute the relevance of the outputs layer\n",
    "        # print(f\"outputs.shape: {outputs.shape}\")\n",
    "        # print(f\"T.shape: {T.shape}\")\n",
    "        R = outputs * T  # Element-wise multiplication\n",
    "        # print(f\"R.shape: {R.shape}\")\n",
    "\n",
    "        Z = torch.nn.functional.linear(batch_activations, model.fc.weight)\n",
    "        # print(f\"Z.shape: {Z.shape}\")\n",
    "\n",
    "        # print(f\"model.fc.weight.shape: {model.fc.weight.shape}\")\n",
    "\n",
    "        # print(Z[0])\n",
    "        # print(outputs[0])\n",
    "\n",
    "        S = R / Z\n",
    "        # print(S1[0])\n",
    "\n",
    "        C = torch.mm(S, model.fc.weight)\n",
    "        # C2 = torch.mm(S2, model.fc.weight)\n",
    "\n",
    "        # print(C2[0] - C1[0])\n",
    "\n",
    "        # print(f\"C.shape: {C2.shape}\")\n",
    "\n",
    "    return C\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximation_lrp_fc_layer(model, dataloader):\n",
    "\n",
    "    for idx, (inputs, _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        T = torch.eye(outputs.size(-1)).to(DEVICE)\n",
    "        T = T[\n",
    "            outputs.argmax(dim=1)\n",
    "        ]  # Select the row from the identity matrix that corresponds to the outputs's highest logit\n",
    "        # R = outputs * T  # Element-wise multiplication\n",
    "\n",
    "        # Z = torch.nn.functional.linear(batch_activations, model.fc.weight)\n",
    "\n",
    "        # S = (output * T) / outputs = T\n",
    "\n",
    "        # C = torch.mm(S, model.fc.weight)\n",
    "        C = torch.mm(T, model.fc.weight)\n",
    "\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This proves that for the last fc layer specifically there is no need to compute the linear transformation of the activations/inputs to the weights' space. Because this gives us the target (which is already known)\n",
    "\n",
    "So, I can just "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = original_lrp_fc_layer(model, dl[\"forget\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "relevance_per_neuron = torch.sum(C, dim=0)\n",
    "print(relevance_per_neuron.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0672e-01, -7.8033e-02, -5.6061e-01,  3.3651e-01, -2.8531e-01,\n",
       "         1.9930e-01, -6.3308e-02,  4.0247e-01, -1.0779e-01, -3.6619e-01,\n",
       "        -4.1586e-01, -2.4042e-01,  3.0840e-01,  4.7466e-01, -1.6652e-01,\n",
       "        -1.8996e-01,  2.6708e-01,  2.9191e-01, -5.8430e-01,  1.3592e-01,\n",
       "         3.9389e-01, -4.7251e-02, -2.7985e-01, -1.4802e-01, -4.5134e-01,\n",
       "         1.9388e-01,  3.5965e-01,  5.0864e-01, -2.1801e-01, -4.3966e-01,\n",
       "         9.9386e-02, -8.1007e-02,  4.5876e-01, -3.0670e-02, -8.0710e-01,\n",
       "         2.3430e-01,  4.2383e-01,  1.5230e-01,  7.5739e-02, -8.9421e-02,\n",
       "        -3.3195e-01,  5.3842e-02,  3.5124e-01, -1.9192e-01, -1.1338e-02,\n",
       "        -1.5301e-01,  2.1557e-02,  2.1885e-01,  3.1724e-01, -6.0119e-01,\n",
       "         4.7164e-01,  2.3765e-01,  4.6690e-01,  1.0392e-01, -4.7191e-01,\n",
       "         1.6880e-01, -1.7206e-01,  1.6652e-02,  8.2775e-01, -1.1498e-01,\n",
       "        -1.9668e-01,  2.3255e-01,  3.3301e-02,  2.0710e-01, -1.0032e-01,\n",
       "         1.5225e-01, -2.4244e-01,  3.9236e-01, -2.4324e-01,  6.2811e-01,\n",
       "         1.6966e-01, -1.7248e-01, -1.0389e-02,  3.6549e-01,  1.0874e-01,\n",
       "         1.9732e-01,  4.3677e-02, -2.4402e-01,  3.3873e-01,  7.6104e-02,\n",
       "         1.8837e-01,  3.8244e-01,  1.7263e-01, -1.2881e-04, -8.9372e-02,\n",
       "         1.6543e-01,  7.8679e-01, -5.0224e-01, -3.1840e-01,  3.5589e-01,\n",
       "         5.6834e-01,  8.5294e-01,  4.3900e-02, -2.9235e-01,  6.3866e-01,\n",
       "         5.8256e-02,  5.8466e-01, -1.3494e-02,  1.2815e-01, -1.6769e-01,\n",
       "         2.0274e-02,  4.6505e-01, -1.3416e-01, -1.5552e-01,  7.4998e-02,\n",
       "         2.4631e-01, -2.8357e-01,  4.7334e-01, -3.0777e-01,  2.7043e-01,\n",
       "        -2.0169e-02,  6.7339e-01, -5.1997e-01,  4.9755e-01, -2.7742e-01,\n",
       "         1.3954e-02, -5.8021e-01,  3.7768e-01,  7.5515e-01, -3.6628e-01,\n",
       "         1.8077e-02,  1.1546e-01, -9.6264e-02,  1.7933e-01,  2.2796e-02,\n",
       "         3.4036e-01, -2.6294e-01,  1.2014e-01,  3.2552e-02,  2.4295e-01,\n",
       "         4.5554e-02, -8.3084e-02,  1.9213e-01,  1.9539e-01,  1.3276e-01,\n",
       "         1.6056e-01, -7.2375e-01,  2.0359e-02,  3.7177e-01, -2.6259e-01,\n",
       "        -5.4221e-02,  2.1156e-01,  1.5145e-01, -8.9273e-02, -3.7336e-01,\n",
       "         3.5849e-01,  2.4313e-01,  3.7978e-01, -3.0720e-01, -7.0175e-01,\n",
       "         6.5030e-01, -1.7767e-01, -1.2940e-01,  2.1177e-01, -2.7630e-01,\n",
       "         3.5760e-01, -4.4809e-01, -3.1868e-01,  5.5052e-01, -3.2045e-01,\n",
       "        -5.7971e-01, -2.8618e-01,  6.8195e-01, -4.7500e-01, -7.4720e-02,\n",
       "        -5.8972e-01,  2.5201e-01,  4.6294e-01, -2.4741e-01, -1.8301e-01,\n",
       "         2.7048e-01,  4.2074e-01, -1.2123e-01, -3.2588e-01,  2.3040e-01,\n",
       "         1.7846e-01, -2.3532e-01,  7.9187e-01, -3.9815e-01, -5.6265e-01,\n",
       "         2.3480e-02, -1.2043e-02, -6.0496e-01,  3.5650e-01,  3.2421e-01,\n",
       "         1.8394e-01, -4.3405e-01,  3.9434e-01,  3.3573e-01,  4.2756e-02,\n",
       "         2.6111e-01,  8.0871e-01, -4.9847e-02,  2.7546e-01, -3.7031e-02,\n",
       "         3.1157e-02,  6.6899e-01,  1.0248e-01, -1.9532e-01, -2.8551e-01,\n",
       "         5.7046e-01, -1.0615e-01, -2.5710e-02,  2.5981e-01,  2.7554e-01,\n",
       "         2.5717e-02,  5.8849e-01, -8.3235e-03, -2.5383e-01,  1.8317e-01,\n",
       "        -8.6521e-02, -2.2515e-01, -2.7719e-01,  5.6167e-01,  4.2178e-01,\n",
       "        -4.6250e-01,  4.1005e-01,  3.5227e-01,  7.1231e-01,  5.5597e-02,\n",
       "        -9.6121e-01, -4.4742e-02, -3.6115e-01,  1.4417e-01,  1.4211e-01,\n",
       "        -2.4637e-01, -2.1291e-01,  5.6005e-01,  5.9681e-01,  1.7532e-01,\n",
       "        -3.3271e-01,  4.1527e-01,  7.8537e-02,  1.8531e-01, -3.7726e-01,\n",
       "         4.4749e-01,  5.3715e-02,  3.1919e-01, -1.8821e-01, -3.7426e-01,\n",
       "        -2.5281e-01, -1.6078e-01,  4.2907e-02, -1.8229e-01,  3.2929e-01,\n",
       "        -1.1278e-01, -5.9756e-01, -1.4744e-01, -1.5639e-01,  2.3317e-01,\n",
       "        -5.8652e-01,  6.6230e-01, -1.0607e-01, -5.6833e-01,  5.0981e-01,\n",
       "         1.0590e-01,  4.4262e-01,  2.2053e-02,  2.9499e-01, -1.8931e-01,\n",
       "        -9.2804e-02, -8.0282e-01, -4.8770e-01, -3.0672e-01,  7.5246e-02,\n",
       "         1.3865e-01, -4.4946e-01,  3.8045e-01, -3.2100e-01, -2.0560e-01,\n",
       "        -9.6619e-02, -2.3762e-01, -3.4831e-01, -1.4785e-01,  5.1628e-01,\n",
       "        -3.3331e-02,  1.4435e-01,  6.2245e-01,  3.6024e-01,  7.0247e-01,\n",
       "         4.0951e-02, -5.2500e-01, -4.0895e-01,  6.8927e-01, -4.3019e-01,\n",
       "         1.3497e-01, -7.0952e-01,  3.6056e-01,  2.0552e-01,  3.1628e-01,\n",
       "         9.4418e-02, -6.0450e-02, -4.4146e-01, -5.6139e-01, -2.4817e-01,\n",
       "         1.6621e-01,  4.9643e-01, -4.8208e-01,  3.9650e-01, -2.2067e-01,\n",
       "         1.6282e-01, -8.1148e-01, -6.4044e-01, -3.6786e-01, -3.5478e-01,\n",
       "        -3.0405e-01,  3.6575e-01,  1.6506e-01,  3.5296e-01,  3.1577e-01,\n",
       "        -4.0169e-01,  3.9788e-01, -2.4378e-01,  8.3297e-02,  3.4097e-01,\n",
       "         6.5682e-01, -4.7397e-01, -9.6216e-01, -1.0000e+00, -5.6285e-01,\n",
       "        -7.7756e-01,  4.2631e-01, -4.0882e-01, -3.2346e-01, -1.6893e-01,\n",
       "        -2.0680e-01,  5.7401e-01, -6.9603e-01, -3.7419e-01,  3.7515e-01,\n",
       "        -6.0034e-02,  1.9059e-01,  3.7137e-01, -6.1652e-01, -5.6081e-01,\n",
       "        -5.6021e-01,  1.5148e-01,  3.8521e-01,  3.9656e-01,  5.1811e-02,\n",
       "        -1.6319e-01,  1.5543e-01,  3.0197e-01,  2.9390e-02,  5.2382e-01,\n",
       "         2.3710e-01, -1.6437e-01, -4.5725e-01, -3.3980e-01,  2.5943e-01,\n",
       "         2.1913e-01, -2.6926e-01,  2.9186e-02, -9.5777e-02, -4.5674e-02,\n",
       "         2.7900e-01, -2.3681e-01,  5.6875e-01,  2.4307e-01,  2.0343e-01,\n",
       "         3.4548e-01,  1.3700e-01,  4.6401e-01,  3.6408e-01, -1.4394e-01,\n",
       "         6.9771e-02, -2.4112e-01, -1.9366e-01,  1.5216e-01,  1.4705e-01,\n",
       "         2.7739e-02,  1.2804e-01,  1.5028e-01,  2.6603e-01,  2.6230e-01,\n",
       "         1.2791e-01,  2.0475e-02, -3.8185e-01,  3.2234e-01,  5.9551e-01,\n",
       "         5.4935e-01, -1.6494e-01,  3.2784e-01,  8.1967e-01, -2.5817e-01,\n",
       "         1.2749e-01, -5.4181e-02, -5.9914e-02, -2.9416e-01,  3.8544e-01,\n",
       "        -2.4166e-01,  5.3532e-01,  2.9540e-01, -3.4160e-01, -2.8190e-01,\n",
       "         1.2612e-01,  3.7937e-01,  2.4288e-01,  2.9522e-01, -6.5421e-01,\n",
       "        -2.8226e-01, -4.1358e-01, -8.1454e-02, -5.2346e-01,  1.4497e-01,\n",
       "         2.1101e-01,  2.0584e-02, -2.6454e-02,  3.8357e-01, -1.3041e-01,\n",
       "        -2.8870e-01,  6.9089e-01, -2.8438e-02, -1.7006e-01,  2.4310e-01,\n",
       "         2.3471e-01, -1.4404e-01, -2.7764e-01,  2.7874e-02,  3.6223e-01,\n",
       "         3.7165e-01, -4.8051e-01,  1.6630e-01,  9.8105e-02, -2.9037e-02,\n",
       "         1.4566e-01, -1.5452e-01,  7.5010e-01, -4.3779e-01,  1.4144e-01,\n",
       "         6.3546e-01,  8.4254e-01, -4.3110e-01, -1.7252e-01,  7.6478e-01,\n",
       "        -1.9050e-01, -4.2475e-01,  3.7316e-01,  2.3562e-01,  5.2797e-02,\n",
       "        -3.2918e-01, -2.8426e-01,  2.6832e-01,  8.4734e-02,  9.7427e-02,\n",
       "        -1.0149e-02,  5.7922e-01,  1.0024e-01, -1.3884e-01, -2.7927e-01,\n",
       "         1.8743e-01, -4.9647e-02, -4.1447e-01, -7.1996e-01, -4.2751e-01,\n",
       "        -4.9038e-02,  1.0568e-01,  2.1783e-01, -2.9098e-01, -5.1043e-01,\n",
       "        -1.3019e-01, -4.9841e-02,  1.4536e-02,  4.6261e-01, -2.3085e-01,\n",
       "        -3.4296e-01,  6.5061e-01, -3.9354e-01, -1.3593e-01,  1.3911e-01,\n",
       "         1.6657e-01,  3.6172e-01,  9.2849e-02, -2.2708e-02,  4.6375e-01,\n",
       "         4.0017e-01,  1.2936e-01,  8.9452e-02, -5.5145e-01,  4.9340e-01,\n",
       "         2.6511e-01,  1.0000e+00, -1.6911e-01,  3.5713e-02,  4.4010e-01,\n",
       "         3.6344e-02,  3.4262e-02,  4.6918e-01, -1.3892e-01,  2.9813e-01,\n",
       "         7.2534e-02, -1.7626e-01,  7.1817e-01,  2.2791e-01,  4.0399e-01,\n",
       "         1.5720e-01,  5.9506e-01,  1.4037e-01, -8.6345e-02, -3.1249e-01,\n",
       "        -3.8973e-01,  5.1945e-01, -2.7364e-01, -2.8552e-01,  1.7344e-01,\n",
       "        -3.2581e-01,  3.5445e-01, -2.3357e-01, -7.3744e-02, -5.7933e-01,\n",
       "        -2.3011e-01, -3.1678e-01], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_relevance = (relevance_per_neuron - relevance_per_neuron.min()) / (relevance_per_neuron.max() - relevance_per_neuron.min())\n",
    "normalized_relevance = (normalized_relevance * 2) - 1\n",
    "normalized_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(194, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mask = torch.where(relevance_per_neuron > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "count_ones = torch.sum(mask)\n",
    "print(count_ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "C_appr = approximation_lrp_fc_layer(model, dl[\"forget\"])\n",
    "relevance_per_neuron_appr = torch.sum(C_appr, dim=0)\n",
    "normalized_relevance_appr = (relevance_per_neuron - relevance_per_neuron.min()) / (\n",
    "    relevance_per_neuron.max() - relevance_per_neuron.min()\n",
    ")\n",
    "normalized_relevance_appr = (normalized_relevance_appr * 2) - 1\n",
    "\n",
    "rpn_diff = normalized_relevance - normalized_relevance_appr\n",
    "sum_diff = torch.sum(rpn_diff)\n",
    "print(sum_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([92, 512])\n"
     ]
    }
   ],
   "source": [
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(rpn_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(rpn_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MaUn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
