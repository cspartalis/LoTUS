{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cspartalis/anaconda3/envs/MaUn/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "from data_utils import UnlearningDataLoader\n",
    "\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from config import set_config\n",
    "from data_utils import UnlearningDataLoader\n",
    "from eval import (\n",
    "    compute_accuracy,\n",
    "    get_forgetting_rate,\n",
    "    get_js_div,\n",
    "    get_l2_params_distance,\n",
    "    mia,\n",
    ")\n",
    "from mlflow_utils import mlflow_tracking_uri\n",
    "from models import VGG19, AllCNN, ResNet18, ViT\n",
    "from seed import set_seed\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Args\n",
    "run_id = \"1bcdd3b016d14404ab22c476184bff75\"\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# Load params from retraining run\n",
    "retrain_run = mlflow.get_run(run_id)\n",
    "seed = int(retrain_run.data.params[\"seed\"])\n",
    "dataset = retrain_run.data.params[\"dataset\"]\n",
    "model_str = retrain_run.data.params[\"model\"]\n",
    "batch_size = int(retrain_run.data.params[\"batch_size\"])\n",
    "epochs_to_retrain = int(retrain_run.data.metrics[\"best_epoch\"])\n",
    "loss_str = retrain_run.data.params[\"loss\"]\n",
    "optimizer_str = retrain_run.data.params[\"optimizer\"]\n",
    "momentum = float(retrain_run.data.params[\"momentum\"])\n",
    "weight_decay = float(retrain_run.data.params[\"weight_decay\"])\n",
    "acc_forget_retrain = int(retrain_run.data.metrics[\"acc_forget\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "UDL = UnlearningDataLoader(dataset, batch_size, seed)\n",
    "dl, dataset_sizes = UDL.load_data()\n",
    "num_classes = len(UDL.classes)\n",
    "input_channels = UDL.input_channels\n",
    "image_size = UDL.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 6/6 [00:01<00:00,  4.12it/s]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet18(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model architecture\n",
    "if model_str == \"resnet18\":\n",
    "    model = ResNet18(input_channels, num_classes)\n",
    "elif model_str == \"allcnn\":\n",
    "    model = AllCNN(input_channels, num_classes)\n",
    "elif model_str == \"vgg19\":\n",
    "    model = VGG19(input_channels, num_classes)\n",
    "elif model_str == \"vit\":\n",
    "    model = ViT(image_size=image_size, num_classes=num_classes)\n",
    "else:\n",
    "    raise ValueError(\"Model not supported\")\n",
    "# Load the original model\n",
    "model = mlflow.pytorch.load_model(f\"{retrain_run.info.artifact_uri}/original_model\")\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def lrp_fc_layer(model, dataloader):\n",
    "    # Define a hook function to get the activations\n",
    "    def get_activations_hook(module, input, output):\n",
    "        activations = input[0].detach().cpu().numpy()\n",
    "        activations_hook.append(activations)\n",
    "\n",
    "\n",
    "    activations_hook = []\n",
    "    model.fc.register_forward_hook(get_activations_hook)\n",
    "\n",
    "    for idx, (inputs, _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get the activations\n",
    "        batch_activations = activations_hook[idx]\n",
    "        batch_activations = torch.from_numpy(batch_activations).to(DEVICE)\n",
    "        # print(f\"batch_activations.shape: {batch_activations.shape}\")\n",
    "\n",
    "        T = torch.eye(outputs.size(-1)).to(DEVICE)\n",
    "        T = T[outputs.argmax(dim=1)] # Select the row from the identity matrix that corresponds to the outputs's highest logit\n",
    "\n",
    "        # Compute the relevance of the outputs layer\n",
    "        # print(f\"outputs.shape: {outputs.shape}\")\n",
    "        # print(f\"T.shape: {T.shape}\")\n",
    "        R = outputs * T  # Element-wise multiplication\n",
    "        # print(f\"R.shape: {R.shape}\")\n",
    "\n",
    "        Z = torch.nn.functional.linear(batch_activations, model.fc.weight)\n",
    "        # print(f\"Z.shape: {Z.shape}\")\n",
    "\n",
    "        # print(f\"model.fc.weight.shape: {model.fc.weight.shape}\")\n",
    "\n",
    "        # print(Z[0])\n",
    "        # print(outputs[0])\n",
    "\n",
    "        S1 = R / Z\n",
    "        # print(S1[0])\n",
    "\n",
    "        C1 = torch.mm(S1, model.fc.weight)\n",
    "        # C2 = torch.mm(S2, model.fc.weight)\n",
    "\n",
    "        # print(C2[0] - C1[0])\n",
    "\n",
    "        # print(f\"C.shape: {C2.shape}\")\n",
    "\n",
    "    return C1    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This proves that for the last fc layer specifically there is no need to compute the linear transformation of the activations/inputs to the weights' space. Because this gives us the target (which is already known)\n",
    "\n",
    "So, I can just "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = lrp_fc_layer(model, dl[\"forget\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "relevance_per_neuron = torch.sum(C, dim=0)\n",
    "print(relevance_per_neuron.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.6443e-01,  3.0196e-01, -3.4898e-01,  3.0182e-01,  7.7416e-02,\n",
       "         2.7558e-01,  2.2935e-01,  6.2738e-02,  1.3916e-01, -3.6274e-01,\n",
       "        -1.1860e-01, -2.5921e-01,  7.5304e-01,  2.4125e-01,  3.0494e-01,\n",
       "        -1.0073e-02,  1.1075e-01,  5.0309e-03, -4.1824e-01,  4.4073e-02,\n",
       "         3.7878e-01,  2.5172e-03, -2.3915e-01, -6.6012e-02, -1.8909e-01,\n",
       "         1.3688e-02,  8.5580e-03,  2.9859e-01, -2.3933e-01, -4.6247e-01,\n",
       "        -1.0160e-01, -1.0404e-01,  3.0131e-01, -3.0913e-02, -7.7992e-01,\n",
       "         5.5145e-01,  1.3846e-01, -3.3275e-01,  3.3197e-01,  2.3156e-01,\n",
       "        -1.8119e-01,  2.7211e-02,  2.5591e-01, -7.4714e-02,  1.3309e-02,\n",
       "         4.9302e-01,  1.8665e-01,  1.7270e-01,  1.5868e-01, -4.8621e-01,\n",
       "         1.4368e-01,  4.7177e-03,  8.9674e-01,  4.0898e-01, -4.1888e-01,\n",
       "        -1.5632e-02, -1.4628e-01,  1.7971e-01,  6.4151e-01,  8.0303e-02,\n",
       "        -2.0325e-01,  5.4051e-01,  1.2389e-01,  2.7504e-01, -3.2037e-01,\n",
       "         1.6246e-01, -7.8501e-02,  9.9449e-02,  5.5787e-02,  3.9739e-01,\n",
       "         4.2820e-01, -7.6187e-04,  1.7429e-01,  1.1651e-01,  2.6711e-01,\n",
       "        -7.4489e-02,  9.5661e-02, -4.9590e-02,  3.1193e-01,  1.8369e-01,\n",
       "         1.3163e-01,  4.0674e-02,  1.7589e-01,  3.0898e-01,  9.4622e-02,\n",
       "         3.0239e-01,  7.0938e-01, -1.3113e-01,  1.0060e-01,  1.6623e-01,\n",
       "         5.3460e-01,  1.0000e+00,  2.1487e-01, -2.5586e-01,  5.1908e-01,\n",
       "         6.0430e-02,  2.7194e-01, -2.0916e-01,  1.0013e-01,  2.2805e-01,\n",
       "        -1.1211e-01,  2.6479e-01, -2.2291e-01, -1.6260e-02,  1.6090e-01,\n",
       "        -6.1307e-02, -2.7323e-01, -1.3843e-02, -3.6209e-01,  5.4884e-01,\n",
       "         3.5500e-01,  4.8504e-01, -2.7052e-01,  4.5432e-01, -2.2983e-01,\n",
       "        -2.3844e-02, -7.2586e-01,  3.4228e-01,  6.7010e-01, -1.0076e-01,\n",
       "        -4.3466e-03,  3.5544e-01, -3.4979e-02,  5.3226e-01,  3.4431e-01,\n",
       "         5.6122e-02, -1.3301e-01,  1.0582e-01,  3.7999e-01,  3.6508e-01,\n",
       "         3.3203e-01,  2.4995e-01,  3.5394e-01, -1.4199e-01,  1.1470e-01,\n",
       "         2.3895e-01, -7.0823e-01, -8.5486e-03,  1.8717e-01,  1.8736e-01,\n",
       "         1.9775e-01,  1.3831e-01, -9.8656e-02,  1.4221e-01, -1.2949e-02,\n",
       "         3.7790e-01,  3.5627e-01,  3.3334e-01, -4.6288e-01, -3.2067e-01,\n",
       "         6.4025e-01, -3.1967e-01, -1.1778e-01,  2.1111e-01, -3.6479e-02,\n",
       "         1.4496e-01, -1.3882e-01, -2.8347e-02,  3.8491e-01, -5.3564e-02,\n",
       "        -1.8681e-01, -2.3932e-01,  5.1162e-01, -1.2422e-01, -1.5487e-01,\n",
       "        -3.2638e-01,  2.8496e-01,  7.5711e-03, -1.5128e-01, -1.2770e-01,\n",
       "         2.5227e-01,  4.8565e-01, -9.6301e-02, -5.6628e-01,  1.9741e-01,\n",
       "         6.4046e-01,  3.1512e-01,  7.8726e-01, -1.6929e-01, -2.1612e-01,\n",
       "         2.0545e-01, -1.0542e-01, -4.9041e-01,  2.1608e-01,  4.7438e-01,\n",
       "         4.1983e-02, -2.8370e-01,  7.4071e-01,  2.1995e-01,  2.2028e-02,\n",
       "         1.5294e-01,  7.1528e-01,  2.5790e-01,  3.6803e-01, -1.1261e-01,\n",
       "         1.3744e-01,  4.9870e-01, -1.1389e-01, -2.5673e-01, -1.6194e-01,\n",
       "         8.1145e-01, -2.7446e-01,  1.1428e-01,  2.8587e-01, -1.9187e-02,\n",
       "        -5.5798e-02,  6.9614e-01, -2.0330e-01,  2.9051e-01, -8.5011e-02,\n",
       "         2.3958e-01,  4.3386e-02, -1.9969e-01,  4.9830e-01,  1.2497e-01,\n",
       "        -2.7024e-01,  3.0950e-01,  7.7365e-01,  4.8119e-01,  1.1199e-02,\n",
       "        -8.1925e-01, -1.6718e-01, -1.6075e-01,  2.3923e-01, -6.3148e-02,\n",
       "        -1.2347e-01, -2.7199e-01,  3.3416e-01,  3.7225e-01,  4.2561e-01,\n",
       "         7.9341e-02,  4.1054e-01,  6.5893e-01,  2.7855e-01,  1.5160e-02,\n",
       "         2.9048e-01,  8.7995e-02,  5.8926e-01, -3.4549e-01, -1.3201e-01,\n",
       "        -4.1668e-01, -3.1414e-01, -2.2377e-01, -4.5072e-01,  2.9839e-01,\n",
       "        -1.4069e-01, -5.1893e-01, -2.1475e-01, -1.8169e-01,  2.1766e-02,\n",
       "        -3.8586e-01,  3.6897e-01,  3.0157e-02, -4.6799e-01,  5.3422e-01,\n",
       "        -1.7362e-01,  3.2116e-01,  1.6536e-01,  6.9741e-02,  3.7165e-02,\n",
       "         6.6476e-02, -6.9942e-01, -4.4045e-01, -2.7421e-02,  3.5552e-01,\n",
       "         1.0309e-01, -9.8803e-02,  3.9760e-01, -1.8946e-01,  2.9394e-02,\n",
       "        -2.8493e-01,  2.0347e-01,  2.6126e-01,  2.1957e-01,  3.8539e-01,\n",
       "         1.0632e-01,  7.1722e-03,  6.5738e-01,  1.3711e-01,  3.7050e-01,\n",
       "        -6.1607e-04, -2.9580e-01, -2.2116e-01,  4.9710e-01, -1.4395e-01,\n",
       "         4.5968e-01, -5.8485e-01,  2.2939e-01,  6.0952e-01,  2.0340e-01,\n",
       "         8.2210e-02, -3.1538e-02,  1.1560e-02, -1.8917e-01,  2.5137e-01,\n",
       "         6.0486e-02,  3.8991e-01, -3.9586e-02, -3.7346e-02, -2.1167e-02,\n",
       "         1.0962e-01, -6.9963e-01, -5.4491e-01, -2.0883e-01, -3.5644e-01,\n",
       "        -2.6240e-01,  1.4189e-01, -1.8194e-01, -1.0129e-02,  2.6700e-01,\n",
       "        -5.4422e-01,  4.2933e-01, -4.1093e-01,  5.1424e-01,  2.7293e-01,\n",
       "         6.2776e-01, -3.6780e-01, -1.0000e+00, -8.9186e-01, -4.2470e-01,\n",
       "        -5.8047e-01,  4.7849e-01, -1.0363e-01, -2.9465e-01,  3.4197e-01,\n",
       "        -1.0500e-01,  7.0285e-01, -7.5703e-01, -5.6975e-01,  3.2359e-01,\n",
       "         2.1011e-01,  6.9148e-02,  1.4828e-01, -4.8520e-01, -7.6246e-01,\n",
       "        -6.7343e-01,  5.5625e-01,  3.8207e-01,  3.5168e-01, -2.1537e-01,\n",
       "         8.7996e-02, -7.7905e-02,  1.7850e-01,  7.9486e-02,  2.7612e-01,\n",
       "         1.3325e-01, -1.0391e-01, -9.3395e-02, -1.7712e-01,  2.2746e-01,\n",
       "        -9.9630e-02, -1.6129e-01,  5.2695e-01,  1.6727e-01,  2.7814e-01,\n",
       "         6.2272e-01, -1.2631e-01,  2.1792e-01,  2.9691e-01,  2.6201e-01,\n",
       "         2.8827e-01,  1.0411e-01,  5.1989e-01,  2.3146e-01, -1.5423e-01,\n",
       "         7.6350e-02, -1.9432e-01, -2.0722e-01,  4.0214e-02,  2.5792e-02,\n",
       "         5.7410e-01, -8.8532e-02, -3.6075e-02, -7.2021e-03,  6.4134e-01,\n",
       "         3.2215e-02,  3.3415e-01, -1.8544e-01,  3.1406e-01,  3.9645e-01,\n",
       "         4.0743e-01,  1.5421e-01,  2.0058e-01,  5.7826e-01,  1.1689e-01,\n",
       "         1.8150e-01,  4.5574e-01,  3.0286e-01, -1.6091e-01,  6.5989e-01,\n",
       "        -7.3665e-02,  2.0332e-01,  5.4287e-02, -9.8168e-02, -3.3608e-01,\n",
       "         2.3263e-01,  3.4208e-01,  2.8579e-01,  4.0058e-01, -6.2207e-01,\n",
       "        -6.7098e-02, -2.3891e-01, -3.2429e-01, -2.3106e-01,  6.1126e-01,\n",
       "         7.0290e-02,  2.3076e-01, -9.2413e-02,  2.2190e-01,  1.2808e-01,\n",
       "        -3.9323e-01,  5.7315e-01,  1.9359e-01,  1.3988e-01,  1.4092e-01,\n",
       "         1.7103e-01, -2.4210e-01, -1.0124e-01, -2.4304e-01,  2.9897e-01,\n",
       "         2.1507e-01, -4.1173e-01,  1.7283e-01, -1.4699e-01, -2.2410e-01,\n",
       "         6.1014e-01, -7.8159e-02,  3.2196e-01, -2.8161e-01,  1.9032e-01,\n",
       "         2.9862e-01,  7.5169e-01, -5.6632e-01,  3.3447e-01,  5.9045e-01,\n",
       "         1.1666e-01, -2.9474e-01,  8.3037e-02,  4.3326e-01, -3.0150e-01,\n",
       "        -1.3824e-01,  1.4588e-01, -8.6910e-02, -5.2022e-02,  4.0824e-01,\n",
       "        -2.0531e-01,  3.0882e-01,  2.1875e-02, -1.3212e-01, -2.7854e-02,\n",
       "         3.2023e-01,  1.2229e-01, -9.0796e-02, -3.0562e-01, -5.6729e-01,\n",
       "        -1.6132e-01,  3.1963e-01,  2.3103e-01, -5.0713e-02, -1.1880e-01,\n",
       "         8.6931e-02,  2.7085e-01, -1.1712e-01,  8.1258e-01, -1.6229e-01,\n",
       "        -4.7465e-01,  5.2562e-01, -2.6771e-01,  1.2837e-01,  2.3117e-01,\n",
       "         3.0239e-01,  2.7155e-01,  4.7601e-01, -3.1531e-01,  4.7798e-01,\n",
       "         2.7561e-01,  4.2890e-01,  4.1918e-02, -4.5350e-01,  5.6781e-01,\n",
       "        -1.4890e-02,  8.0900e-01,  2.7421e-01, -1.2901e-01,  2.8414e-01,\n",
       "         1.6383e-01,  1.6583e-01,  3.7742e-01, -2.1631e-01,  4.8157e-02,\n",
       "         2.7157e-02, -3.2606e-01,  7.2570e-01,  4.0035e-02,  5.8052e-01,\n",
       "         4.2452e-01,  6.1493e-01,  2.9901e-01,  2.2306e-01, -2.3426e-01,\n",
       "         4.5929e-02,  3.7950e-01, -1.7765e-01, -1.4294e-01,  3.0203e-03,\n",
       "         1.2606e-01,  3.2753e-01, -3.2433e-01,  2.5770e-01, -4.7522e-01,\n",
       "         1.0796e-01, -1.8647e-01], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_relevance = (relevance_per_neuron - relevance_per_neuron.min()) / (relevance_per_neuron.max() - relevance_per_neuron.min())\n",
    "normalized_relevance = (normalized_relevance * 2) - 1\n",
    "normalized_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(193, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "mask = torch.where(relevance_per_neuron > 0.5, torch.tensor(1), torch.tensor(0))\n",
    "count_ones = torch.sum(mask)\n",
    "print(count_ones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ουσιαστικά, βρήκαμε τους πιο σημαντικούς νευρώνες από το προτελευταίο layer.\n",
    "Και μπορούμε να πειράξουμε τα βάρη τους στο τελευταιο layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MaUn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
